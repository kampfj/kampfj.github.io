---
layout: single
title: "Algorithms to Live By"
categories: book_post 
---


*Algorithms to Live By*, by Tom Griffiths and Brian Christian, distills complex ideas about computer science and mathematics into approachable concepts in a way that can be appreciated by both an eager student yet to take a college computer science course as well as a PhD student. The first time I read the book, I was an eager pre-frosh who knew nothing about coding, algorithms, or computer science theory. The book served as the impetus for me to register for CIS 110, the introductory computer science course at Penn during my freshman fall. Two years later, after completing much of the core curriculum for the CIS major at Penn, I decided to return back to where I started and revisit *Algorithms to Live By*. 

Before I dive into the book itself, one of the biggest things I learned from rereading it is the power and importance of review. In Judaism and specifically Jewish Study, there is a popular idea of *chazarah*, literally meaning "to go back" or "review". *Chazarah* emphasizes the essential practice of taking a few minutes to go over what one has learned in order to acquire it further and concretize it in the brain. When I read *Algorithms to Live By* almost two years ago, it wasn’t as if I skimmed the book. In fact, I have to this day nearly ten pages of notes that I took while reading it, diligently copying down important ideas in an effort to retain them more effectively. Yet, as I turned the pages during this go-round, I found the ideas not just fascinating, but novel. This time, I didn’t feel the need to take notes. Since I had seen the material before, it was much easier to swallow and stomach. Griffiths and Christian were no longer speaking a foreign language. I saw the book and its contents in a totally new way. 

This leads me to something I’ve continued to observe over my time in college - something I always knew in the back of my head, but something that was hard to actually implement and stay disciplined about: when approaching a hard problem or a dense text, sometimes the best thing to do is to put it away, clear your head, and come back to it later. Often, we get stuck and bogged down in a zone of trying to figure it out. A second look at a problem or a text might open up things you’d never seen before - a word that meant nothing finally clicks in the context of its sentence. A constraint in the problem you couldn’t really understand sheds light on a key observation about the input. It’s important to give yourself this time and space. If you have the luxury, perhaps you’ll want the two years that I gave myself with this book. If it’s a homework assignment or something with a deadline, maybe grab a snack, nap, movie, walk, or shower before coming back to work. But do something. 

The best part of reading for me is when I stumble on a bundle of information or a snippet of writing that really makes me think twice. “Wow, I never thought about it like that” is a truly satisfying moment - I’ve truly learned something I didn’t yet know. Or even better, I’ve learned a way of thinking I had previously been blind to. I want to highlight two of those moments that I had with *Algorithms to Live By*. 

## Explore/Exploit

One of the core themes of the book, as well as computer science in general, is tradeoffs. This might be intuitive - computers are very small machines which perform very complex computations. The challenge for engineers is to fit a ton of power into a finite space with, to be sure, a finite supply of resources like time and money. With that being said, much of the focus of algorithmic design is to limit the total computation done to reach a solution as well as the amount of space used in obtaining it. 

Christian and Griffiths incisively analyze a tradeoff humans make each and every day: how do we balance **exploring** new things and **exploiting** ones we already know and love? In general, the word "exploit" carries a negative connotation, but in this context it has none - all it means is enjoying a resource/activity that you’ve done before. Think about going back to that smokehouse to get your favorite steak, or watching *Friends* all the way through for the sixth time. With *exploiting* a known pleasure, I mean, you can’t go wrong. 

On the other hand, why don’t you check out some of the new shows airing on Netflix? Well, the problem with this exploration is that you might not like them at all. And if you don’t like them, you kinda wasted your time. If you do discover a show that you love, though, you’ve just unlocked a whole new pastime. And who doesn’t want that? 

This tension opens up an entire field of computational theory known as [optimal stopping](https://en.wikipedia.org/wiki/Optimal_stopping). The applications of this theory are not only fascinating, but also extremely relevant to our daily lives. 

What Christian and Griffiths get at so well is that our decisions and tradeoffs in the context of explore/exploit depend mainly on our *interval* in which we are exploring and/or exploiting. In other words, if you’re in Miami for one more day and have one more meal left, you’re probably better off going to that sure-thing steakhouse as opposed to trying out a new place. After all, it’s your last chance to enjoy it before you leave town! 

In the context of the *interval*, Christian and Griffiths reveal how optimal stopping sheds light on human behavior. For example, the tension between explore and exploit can help explain the tendency of a young child to push buttons at random on a telephone just to see what happens, or of a man in an old age home who returns to play the same bingo game every single Sunday for two years running. In our world, the young tend to explore while the elderly exploit. As Christian and Griffiths explain so well:

> “we think of the young as stereotypically fickle; the old, stereotypically set in their ways. In fact, both are behaving completely appropriately with respect to their intervals.” 

# The Prominence of Sorting 

Going through several algorithms courses, one begins to wonder: why is everyone so obsessed with sorting? So much of computer science theory has gone to this topic. While reading this book, I actually had the conscious thought: when was the last time I encountered a list on the Internet that wasn’t sorted? And then I thought about it: Google, eBay, Amazon - literally any online platform automatically gives us our content in some sorted order. Further, it is up to the consumer to decide which sorted order is most preferable. Maybe you want least to most expensive, or highest to lowest rated, or any permutation of these preferences. These luxuries require the ability to sort massive datasets over and over again, very quickly. And as your dataset gets larger and larger, the difference between a quadratic algorithm like `Bubble Sort` and a linearithmic algorithm like `MergeSort` becomes crucial (as an aside, I had never seen the term linearithmic before reading this book - it refers to a function that is both linear and logarithmic in its input - in other words, *f(n) = nlogn*). 

What really got to me is how Christian and Griffiths showed sorting playing out in the real world - namely, March Madness. For those who don't know, March Madness is the NCAA National Tournament that uses a single-elimination, 64-team pool and bracket-style play as well as seedings to determine a national champion in just six rounds. 

What the authors show is that the NCAA bracket format is a partial `MergeSort` - an extremely efficient method of finding a winner in a large pool that’s conveniently a power of two. With each round of the tournamnet, the pool halves. Thus, with *n* teams it will take *logn* rounds to find a winner through a series of head-to-head matchups. The rhythm that is The Round of 64 to The Round of 32 to the Sweet Sixteen to the Elite Eight to the Final Four and finally to the NCAA Championship is no coincidence. It is a result of algorithmic design based on a desire to maximize excitement and accuracy while minimizing overhead in the form of expenses, team travel, and time missed away from school for student-athletes headed toward finals season. In fact, there’s an entire field of sports scheduling that has computer science theory written all over it, and I’m super interested in it. Perhaps more will come on that at a later date. 

The seasoned reader may wonder: if March Madness is essentially one big `MergeSort`, how come it takes logarithmic time (as we showed above) and not linearithmic. After all, we know that the complexity of `MergeSort` is *nlogn*. The answer is that March Madness is an approximate *MergeSort*, not an complete one. It doesn't care about sorting the losing teams - for instance, consider the sixteen losers in the Round of 32. Because our goal is to find an ultimate champion, we discard these teams without regard to how they stack up against one another. To find that out, we'd need a ton more games. Instead, the losers leave the competition and the winners move forward in an effort to reach the 'base case' of one team remaining, who will be crowned the champion. This eliminates the need to do linear work at each level in order to sort completely, and allows us to take complete advantage of the divide and conquer paradigm in order to reach a logarithmic runtime. 

At the end of the day, the fact that the NCAA Tournament which I grew up loving is a partial `MergeSort` and can be understood more completely through the lens of computer science is just...cool. 

I highly recommend this book if you’re looking for a concise dive into the relevance of computer science theory to our daily lives, or just want to learn something new. However, I do think that it does a pretty poor job of communicating the concept of recursion to people who have never seen it before - I think a better and more intuitive explanation can be found [here](https://www.quora.com/How-should-I-explain-recursion-to-a-4-year-old). 

For now, I’m putting *Algorithms to Live By* back on the bookshelf. Who knows? Maybe I’ll pick it up a few years down the line and discover something new. 